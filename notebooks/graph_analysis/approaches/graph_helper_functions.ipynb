{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6875b1a-f1fe-4779-a4bb-17c5a9c8ee70",
   "metadata": {},
   "source": [
    "# Graph Analysis Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17ef2b5-015c-4b1c-886c-ac8553b869e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import sqlalchemy as salc\n",
    "import networkx as nx\n",
    "import json\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908df105-d4d6-48c6-b322-7272cb026dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos(repos, engine):\n",
    "\n",
    "    repo_set = []\n",
    "    repo_git_set = []\n",
    "    for repo_git in repos:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                     SET SCHEMA 'augur_data';\n",
    "                     SELECT \n",
    "                        b.repo_id,\n",
    "                        b.repo_name,\n",
    "                        b.repo_git\n",
    "                    FROM\n",
    "                        repo_groups a,\n",
    "                        repo b\n",
    "                    WHERE\n",
    "                        a.repo_group_id = b.repo_group_id AND\n",
    "                        b.repo_git = \\'{repo_git}\\'\n",
    "            \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            t = conn.execute(repo_query)\n",
    "        results = t.mappings().all()[0]\n",
    "        repo_id = results['repo_id']\n",
    "        repo_git = results['repo_git']\n",
    "        repo_set.append(repo_id)\n",
    "        repo_git_set.append(repo_git)\n",
    "    return repo_set, repo_git_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a5c30d-9e9d-46b2-916a-ff64809bc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_contributors(repo_set, engine):\n",
    "\n",
    "    issue_contrib = pd.DataFrame()\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    ie.cntrb_id,\n",
    "                    ie.action,\n",
    "                    i.issue_id,\n",
    "                    i.created_at\n",
    "                    FROM\n",
    "                    repo r, issues i, issue_events ie\n",
    "                     WHERE\n",
    "                    i.repo_id = \\'{repo_id}\\' AND\n",
    "                    i.repo_id = r.repo_id AND\n",
    "                    i.issue_id = ie.issue_id AND\n",
    "                    ie.action='closed'\n",
    "            \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            df_current_repo = pd.read_sql_query(repo_query, conn)\n",
    "        issue_contrib = pd.concat([issue_contrib, df_current_repo])\n",
    "\n",
    "    issue_contrib = issue_contrib.reset_index()\n",
    "    issue_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    issue_contrib.columns =['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'action', 'issue_id', 'created_at']\n",
    "    return issue_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e43d1c-e03d-4428-a600-dfd40f9ae74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos_outside(engine):\n",
    "\n",
    "    issue_contrib = pd.DataFrame()\n",
    "    repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_name,\n",
    "                    (CASE WHEN REGEXP_LIKE(repo_name, 'https://github.com/open-telemetry/opentelemetry-go|https://github.com/open-telemetry/opentelemetry-specification|https://github.com/open-telemetry/opentelemetry-collector') THEN true ELSE NULL\n",
    "                    END) AS flag\n",
    "                    FROM repo r\n",
    "            \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df_current_repo = pd.read_sql_query(repo_query, conn)\n",
    "        \n",
    "    print(df_current_repo)\n",
    "    return issue_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b397d6d4-320a-4434-a1ff-5bd006eddebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_contributors(repo_set, engine):\n",
    "\n",
    "    pr_contrib = pd.DataFrame()\n",
    "\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    prm.cntrb_id,\n",
    "                    prm.pull_request_id,\n",
    "                    pr.pr_created_at\n",
    "                    FROM\n",
    "                    repo r, pull_request_meta prm, pull_requests pr\n",
    "                    WHERE\n",
    "                    prm.repo_id = \\'{repo_id}\\' AND\n",
    "                    prm.repo_id = r.repo_id AND\n",
    "                    prm.pull_request_id = pr.pull_request_id\n",
    "            \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            df_current_repo = pd.read_sql_query(repo_query, conn)\n",
    "        pr_contrib = pd.concat([pr_contrib, df_current_repo])\n",
    "\n",
    "    pr_contrib = pr_contrib.reset_index()\n",
    "    pr_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    pr_contrib.columns =['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'pull_request_id', 'pr_created_at']\n",
    "\n",
    "    return pr_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87158ad6-c882-4f36-97d3-26f6eaa96dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commit_contributors(repo_set, engine):\n",
    "\n",
    "    commit_contrib = pd.DataFrame()\n",
    "\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    ca.cntrb_id,\n",
    "                    c.cmt_id,\n",
    "                    c.cmt_date_attempted\n",
    "                    FROM\n",
    "                    repo r, commits c, contributors_aliases ca\n",
    "                    WHERE\n",
    "                    c.repo_id = \\'{repo_id}\\' AND\n",
    "                    c.repo_id = r.repo_id and\n",
    "                    c.cmt_committer_email = ca.alias_email\n",
    "            \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            df_current_repo = pd.read_sql_query(repo_query, conn)\n",
    "        commit_contrib = pd.concat([commit_contrib, df_current_repo])\n",
    "\n",
    "    commit_contrib = commit_contrib.reset_index()\n",
    "    commit_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    commit_contrib.columns =['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'cmt_id', 'cmt_date_attempted']\n",
    "\n",
    "    return commit_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a73e2b4-a6de-4048-aab7-bec1bb2aa4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prr_contributors(repo_set, engine):\n",
    "\n",
    "    prr_contrib = pd.DataFrame()\n",
    "\n",
    "    for repo_id in repo_set:\n",
    "        repo_query = salc.sql.text(f\"\"\"\n",
    "                    SET SCHEMA 'augur_data';\n",
    "                    SELECT r.repo_id,\n",
    "                    r.repo_git,\n",
    "                    r.repo_name,\n",
    "                    prr.cntrb_id,\n",
    "                    prr.pull_request_id\n",
    "                    FROM\n",
    "                    repo r, pull_request_reviewers prr\n",
    "                    WHERE\n",
    "                    prr.repo_id = \\'{repo_id}\\' AND\n",
    "                    prr.repo_id = r.repo_id\n",
    "            \"\"\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            df_current_repo = pd.read_sql_query(repo_query, conn)\n",
    "        prr_contrib = pd.concat([prr_contrib, df_current_repo])\n",
    "\n",
    "    prr_contrib = prr_contrib.reset_index()\n",
    "    prr_contrib.drop(\"index\", axis=1, inplace=True)\n",
    "    prr_contrib.columns = ['repo_id', 'repo_git', 'repo_name', 'cntrb_id', 'pull_request_id']\n",
    "\n",
    "    return prr_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb85e401-d717-47a5-8639-cc23555de47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_melted_dfs(df):\n",
    "\n",
    "    df = df.groupby(['org_repo', 'cntrb_id']).size().unstack(fill_value=0)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df_melted = df.melt(['org_repo'], var_name = 'cntrb_id',value_name='number')\n",
    "    df_melted = df_melted[df_melted[df_melted.columns[2]] != 0]\n",
    "\n",
    "    return df_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f88f1f5-9cf6-46ba-8d9b-bed7b45a3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_ranks(graph, top, repo_dict, scores):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method takes in a graph, and returns the nodes ranked by page rank \n",
    "    graph: input graph\n",
    "    top: top number of repos to subset after calculating the page rank\n",
    "    known_repos: list of repository/community names known to us\n",
    "    other_repos: list of repository/community names that we want to determine the importance of\n",
    "    \"\"\"\n",
    "    \n",
    "    pageranks = nx.pagerank(graph, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None)\n",
    "    \n",
    "    scores['page_rank'] = scores['repo'].map(pageranks)\n",
    "    \n",
    "    pr_dicts = collections.defaultdict(dict)\n",
    "    \n",
    "    for key in pageranks:  \n",
    "        for repo_group in repo_dict:\n",
    "            if key in repo_dict[repo_group]:\n",
    "                pr_dicts[repo_group][key] = pageranks[key]\n",
    " \n",
    "    top_repos = collections.defaultdict(dict)\n",
    "    \n",
    "    for pr_dict in pr_dicts:\n",
    "        top_repos[str(pr_dict)] = dict(sorted(pr_dicts[pr_dict].items(), key = itemgetter(1), reverse = True)[:top])\n",
    "            \n",
    "    return top_repos, pageranks, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e532825c-411d-4cce-b659-31ea239ba910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betweenness_centrality(graph, top, repo_dict, scores):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method takes in a graph, and returns the nodes ranked by betweenness centrality scores\n",
    "    graph: input graph\n",
    "    top: top number of repos to subset after calculating the betweenness centrality scores\n",
    "    known_repos: list of repository/community names known to us\n",
    "    other_repos: list of repository/community names that we want to determine the importance of\n",
    "    \"\"\"\n",
    "    \n",
    "    # Betweenness centrality measures the extent to which a node lies on paths between other nodes in the graph. \n",
    "    # Nodes with higher betweenness have more influence within a network. \n",
    "    # Thus repositories with higher centrality scores can thought to be influential in connection to other repositories in the network.\n",
    "    \n",
    "    bw_centrality = nx.betweenness_centrality(graph)\n",
    "\n",
    "    scores['betweenness_centrality'] = scores['repo'].map(bw_centrality)\n",
    "    \n",
    "    bc_dicts = collections.defaultdict(dict)\n",
    "    \n",
    "    for key in bw_centrality:  \n",
    "        for repo_group in repo_dict:\n",
    "            if key in repo_dict[repo_group]:\n",
    "                bc_dicts[repo_group][key] = bw_centrality[key]\n",
    " \n",
    "    top_repos = collections.defaultdict(dict)\n",
    "    \n",
    "    for bc_dict in bc_dicts:\n",
    "        top_repos[str(bc_dict)] = dict(sorted(bc_dicts[bc_dict].items(), key = itemgetter(1), reverse = True)[:top])\n",
    "            \n",
    "    return top_repos, bw_centrality, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e83c78-0404-40cb-8e5e-71a9f11d9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closeness_centrality(graph, top, repo_dict, scores):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method takes in a graph, and returns the nodes ranked by closeness centrality scores\n",
    "    graph: input graph\n",
    "    top: top number of repos to subset after calculating the closeness centrality scores\n",
    "    known_repos: list of repository/community names known to us\n",
    "    other_repos: list of repository/community names that we want to determine the importance of\n",
    "    \"\"\"\n",
    "    \n",
    "    c_centrality = nx.closeness_centrality(graph)\n",
    "    \n",
    "    scores['closeness_centrality'] = scores['repo'].map(c_centrality)\n",
    "    \n",
    "    cc_dicts = collections.defaultdict(dict)\n",
    "    \n",
    "    for key in c_centrality:  \n",
    "        for repo_group in repo_dict:\n",
    "            if key in repo_dict[repo_group]:\n",
    "                cc_dicts[repo_group][key] = c_centrality[key]\n",
    " \n",
    "    top_repos = collections.defaultdict(dict)\n",
    "    \n",
    "    for cc_dict in cc_dicts:\n",
    "        top_repos[str(cc_dict)] = dict(sorted(cc_dicts[cc_dict].items(), key = itemgetter(1), reverse = True)[:top])\n",
    "            \n",
    "    return top_repos, c_centrality, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0379ee41-df81-497c-b87f-587f872ada8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(graph, repo_dict, size, title, weights=None, with_labels=True, alpha=None, edge_color='k'):\n",
    "    \n",
    "    \"\"\"\n",
    "    graph: the networkX graph that we want to plot\n",
    "    known_repos: list of known repos for coloring\n",
    "    other_repos: list of other repos for coloring\n",
    "    size: can be either 'weighted', 'equal' or 'conditional'\n",
    "    When size is 'weighted', the node sizes on the graph are based on the weights provided\n",
    "    When size is 'equal', all nodes are the same size\n",
    "    When size is 'conditional', nodes which belong to the weights array are larger than the rest of the nodes\n",
    "    weights: this decides the size of the nodes in the 'weighted' and 'conditional' type sizes\n",
    "    \n",
    "    here we plot a networkx graph based on the provided parameters\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "\n",
    "    nodes = graph.nodes()\n",
    "    node_colors = []\n",
    "    chosen_colors = random.sample(list(mcolors.TABLEAU_COLORS), len(repo_dict) + 1)\n",
    "    repo_no = 0\n",
    "    \n",
    "    #assign random colors to each repo_group\n",
    "    color_map = collections.defaultdict(str)   \n",
    "    for repo_group in repo_dict:\n",
    "        color_map[repo_group] = chosen_colors[repo_no]\n",
    "        repo_no += 1\n",
    "    color_map['Contributors'] = chosen_colors[-1]\n",
    "\n",
    "    for n in nodes:\n",
    "        color_assigned = False\n",
    "        try:\n",
    "            uuid.UUID(str(n))\n",
    "            node_colors.append(color_map['Contributors'])\n",
    "            continue\n",
    "        except ValueError:\n",
    "            for repo_group in repo_dict:                    \n",
    "                if n in repo_dict[repo_group]:\n",
    "                    node_colors.append(color_map[repo_group])\n",
    "                    color_assigned = True\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        if color_assigned == False:\n",
    "            node_colors.append(\"white\")\n",
    "\n",
    "    if size == 'weighted':\n",
    "        node_sizes = [v * 10000 for v in weights.values()]\n",
    "    elif size == 'conditional':\n",
    "        node_sizes = [1000 if ns in weights else 50 for ns in nodes]\n",
    "    elif size == 'equal':\n",
    "        node_sizes = 300\n",
    "    \n",
    "    for color in color_map:\n",
    "        patches.append(mpatches.Patch(color=color_map[color], label=color))\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "    font = {\"color\": \"k\", \"fontsize\": 15}\n",
    "    \n",
    "    ax.set_title(title, font)\n",
    "    ax.legend(handles=patches)\n",
    "    \n",
    "    nx.draw_networkx(graph, node_color=node_colors, node_size=node_sizes, font_size=9, ax=ax, with_labels=with_labels, alpha=alpha, edge_color=edge_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a562a663-b92e-4323-92a2-7e7c58ed1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_nodes_edges_contributions(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using this function we represent data as a graph where the project repositories are represented by nodes \n",
    "    and the edges are shared contributions between those projects\n",
    "    \"\"\"\n",
    " \n",
    "    # structure of `contributorGraph` =  \n",
    "    # {  \n",
    "    # `contributor1`: [(`repo1`, `contributions by the contributor1 in repo 1`)],  \n",
    "    #  `contributor2`: [(`repo2`, `contributions by the contributor2 in repo 2` ), (`repo1`, `contributions by the contributor2 in repo 1`)]  \n",
    "    # }\n",
    "\n",
    "    contributorGraph = {}\n",
    "    for i, row in df.iterrows():\n",
    "        if row['cntrb_id'] not in contributorGraph:\n",
    "            contributorGraph[row['cntrb_id']] = []\n",
    "        if(row['total_contributions'] > 0):\n",
    "            contributorGraph[row['cntrb_id']].append((row['org_repo'], row['total_contributions']))\n",
    "            \n",
    "    # `contributorGraph`  is a dictionary where each key is a contributor, \n",
    "    #  and the value is a list of repositories the contributor has contributed to and the number of contributions it has made.\n",
    "    \n",
    "    #  \"shared connections\" constitute of commits, PRs, issues* and PR reviews that are made by the same contributor.\n",
    "    #  2 project repositories are \"connected\" if they have a \"shared connection\"** between them. \n",
    "    #  If they have a contributor who makes a commit, PR, issue or PR review in both the repositories, \n",
    "    #  they count as a shared contributor and the repositories are connected. \n",
    "    \n",
    "    commonRepoContributionsByContributor = collections.defaultdict(int)\n",
    "    for key in contributorGraph:\n",
    "        if len(contributorGraph[key])-1 <= 0:\n",
    "            continue\n",
    "        for repoContributionIndex in range(len(contributorGraph[key])-1):\n",
    "            commonRepoContributionsByContributor[(contributorGraph[key][repoContributionIndex][0], contributorGraph[key][repoContributionIndex+1][0])] += contributorGraph[key][repoContributionIndex][1]+contributorGraph[key][repoContributionIndex+1][1]\n",
    "\n",
    "    # `commonRepoContributionsByContributor` is a nested dictionary consisting of dictionaries of repository pairs and their common contributions. \n",
    "    #  structure of `commonRepoContributionsByContributor` =  \n",
    "    #  {  \n",
    "    #  (`repo1, repo2`): `PRs by same authors in repo 1 and repo 2`,  \n",
    "    #  (`repo2, repo4`): `PRs by same authors in repo 2 and repo 4`,  \n",
    "    #  (`repo2, repo5`): `PRs by same authors in repo 2 and repo 5`,   \n",
    "    #   }    \n",
    "    \n",
    "    res = []\n",
    "    for key in commonRepoContributionsByContributor:\n",
    "        res.append(tuple(str(k) for k in list(key)) + (commonRepoContributionsByContributor[key],))\n",
    "        \n",
    "    return res, commonRepoContributionsByContributor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
